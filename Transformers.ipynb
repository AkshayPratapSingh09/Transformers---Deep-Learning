{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers (Encoder-Decoder Arch.)"
      ],
      "metadata": {
        "id": "D402Dl0j5K71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/10/transformer_1.png\" width=500px height=700px/>"
      ],
      "metadata": {
        "id": "67i8WVru5NSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attenttion is all you Need Paper** '2017"
      ],
      "metadata": {
        "id": "mweP36p5527D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`**ANN**` - better for Tabular Data <br>\n",
        "`**CNN**` - better for Image Data <br>\n",
        "`**RNN**` -Better for Sequential Data(Sequence) <br>\n",
        "`**Transformers**` - better for Sequence to Sequence (Text)"
      ],
      "metadata": {
        "id": "k6G-NlzI6BkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impact of Transformers in NLP:\n",
        "\n",
        "\n",
        " ### **Evolution of NLP** -\n",
        "# - **Heuristic Methods**\n",
        "\n",
        "Here are some common heuristic methods used in NLP:\n",
        "\n",
        "## 1.1 Rule-Based Approaches\n",
        "**Regular Expressions (Regex)**: A powerful tool for pattern matching in text, often used for tasks like text extraction, data validation, or string manipulation.\n",
        "\n",
        "**Dictionary/Keyword Matching:** Involves using predefined lists of words or phrases to identify specific entities or concepts in the text (e.g., identifying names, locations, or dates).\n",
        "\n",
        "**Grammar-Based Parsing:** Uses syntactic rules to parse sentences and extract structured information, useful in tasks like Information Extraction (IE).\n",
        "\n",
        "## 1.2. Heuristics for Text Preprocessing\n",
        "**Stopword Removal:** Heuristically identifying and removing common words (like \"the,\" \"is,\" \"in\") that usually carry less meaning in text.\n",
        "\n",
        "**Stemming/Lemmatization:** Reducing words to their base or root form, based on rules or dictionaries, to standardize text (e.g., \"running\" → \"run\").\n",
        "\n",
        "**Tokenization:** Splitting text into words or phrases using simple rules like splitting by spaces or punctuation.\n",
        "\n",
        "## 1.3. Heuristics in Named Entity Recognition (NER)\n",
        "**Capitalization Rules:** Identifying proper nouns (e.g., names of people, places) based on capitalization patterns.\n",
        "\n",
        "**Contextual Rules:** Using surrounding words to infer entity types (e.g., \"Dr.\" before a word likely indicates a person's name).\n",
        "\n",
        "## 1.4. Heuristics in Sentiment Analysis\n",
        "**Sentiment Lexicons:** Using predefined lists of positive and negative words to estimate the sentiment of a sentence or document.\n",
        "\n",
        "**Polarity Shifting:** Applying simple rules to handle negations or modifiers (e.g., \"not good\" → negative sentiment).\n",
        "\n",
        "## 1.5. Heuristics in Machine Translation\n",
        "**Phrase-Based Heuristics:** Translating common phrases directly based on bilingual dictionaries, handling exceptions with simple rules.\n",
        "\n",
        "**Contextual Heuristics:** Adjusting translations based on the context (e.g., different meanings of the same word in different sentences).\n",
        "## 1.6. Heuristics in Topic Modeling\n",
        "\n",
        "**Keyword-Based Topic Identification:** Identifying topics in a document by matching keywords to predefined topic lists.\n",
        "\n",
        "**Frequency-Based Heuristics:** Assuming high-frequency terms in a document likely represent its main topics.\n",
        "\n",
        "## 1.7. Heuristics in Search and Information Retrieval\n",
        "**TF-IDF (Term Frequency-Inverse Document Frequency):** Heuristically ranking documents based on the importance of words within them.\n",
        "\n",
        "**Proximity Heuristics:** Ranking search results based on the closeness of query terms in documents.\n",
        "\n",
        "\n",
        "# **Statistical Machine Learning Models for NLP**\n",
        "\n",
        "## 1. Naive Bayes Classifier\n",
        "- **Description:** A probabilistic model based on Bayes' theorem, which assumes that features (e.g., words in a document) are independent given the class label. Despite its \"naive\" independence assumption, it performs surprisingly well in many NLP tasks.\n",
        "- **Use Cases:** Text classification (e.g., spam detection, sentiment analysis), document categorization.\n",
        "\n",
        "## 2. Logistic Regression\n",
        "- **Description:** A linear model that estimates the probability of a binary outcome (e.g., positive vs. negative sentiment) by fitting a logistic function to the input features. It can be extended to multiclass classification using techniques like one-vs-all.\n",
        "- **Use Cases:** Sentiment analysis, text classification, named entity recognition (NER).\n",
        "\n",
        "## 3. Support Vector Machines (SVM)\n",
        "- **Description:** A powerful model that finds the hyperplane in a high-dimensional space that best separates different classes. It is particularly effective in high-dimensional spaces like those encountered in text data.\n",
        "\n",
        "- **Use Cases:** Text classification, sentiment analysis, topic categorization.\n",
        "\n",
        "## 4. Hidden Markov Models (HMM)\n",
        "\n",
        "- **Description:** A statistical model that represents sequences (e.g., words in a sentence) where the system is assumed to be a Markov process with hidden states. HMMs are often used for tasks where the goal is to predict sequences or uncover hidden states.\n",
        "\n",
        "- **Use Cases:** Part-of-speech tagging, named entity recognition, speech recognition.\n",
        "\n",
        "## 5. Conditional Random Fields (CRF)\n",
        "\n",
        "- **Description:** A discriminative model used for structured prediction, which directly models the conditional probability of the output sequence given the input sequence. It addresses limitations of HMMs by avoiding independence assumptions.\n",
        "\n",
        "- **Use Cases:** Named entity recognition, part-of-speech tagging, chunking.\n",
        "\n",
        "## 6. Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "- **Description:** A generative probabilistic model used for topic modeling, which assumes that documents are mixtures of topics and topics are mixtures of words. It uses statistical inference to discover the underlying topics in a corpus.\n",
        "\n",
        "- **Use Cases:** Topic modeling, document classification, content recommendation.\n",
        "\n",
        "## 7. K-Nearest Neighbors (KNN)\n",
        "\n",
        "- **Description:** A simple, non-parametric model that classifies a data point based on the majority label of its nearest neighbors in the feature space. It relies on distance metrics to find the closest neighbors.\n",
        "\n",
        "- **Use Cases:** Text classification, document similarity, recommendation systems.\n",
        "\n",
        "## 8. Decision Trees and Random Forests\n",
        "\n",
        "- **Description:** Decision trees are hierarchical models that split data based on feature values to make predictions. Random forests are an ensemble of decision trees, combining their outputs to improve robustness and accuracy.\n",
        "\n",
        "- **Use Cases:** Text classification, sentiment analysis, feature selection.\n",
        "\n",
        "## 9. Gaussian Mixture Models (GMM)\n",
        "- **Description:** A probabilistic model that represents a mixture of multiple Gaussian distributions. GMMs are used for clustering tasks where the data is assumed to be generated from a mixture of several Gaussian distributions.\n",
        "\n",
        "- **Use Cases:** Document clustering, topic modeling, anomaly detection.\n",
        "\n",
        "## 10. Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)\n",
        "- **Description:** RNNs are a type of neural network designed for sequential data, where connections between nodes form a directed cycle. LSTMs are a variant of RNNs that address the problem of vanishing gradients, making them effective for capturing long-range dependencies.\n",
        "\n",
        "- **Use Cases:** Text generation, language modeling, machine translation, speech recognition.\n",
        "\n",
        "## **Embedding Models**\n",
        "\n",
        "# Problems That Embeddings Solve in NLP\n",
        "\n",
        "## 1. High Dimensionality\n",
        "- **Problem:** Traditional representations of text, like the Bag of Words (BoW) or one-hot encoding, result in extremely high-dimensional vectors, especially for large vocabularies. This makes the models computationally expensive and difficult to generalize.\n",
        "- **Solution by Embeddings:** Embeddings reduce the dimensionality by representing words, sentences, or documents as dense vectors in a lower-dimensional space, making the models more efficient and scalable.\n",
        "\n",
        "## 2. Sparsity\n",
        "- **Problem:** One-hot vectors and BoW models are sparse, meaning most of the vector elements are zeros. This sparsity can make learning patterns from data difficult and inefficient.\n",
        "- **Solution by Embeddings:** Embeddings provide dense representations, where each vector has non-zero values, capturing more meaningful information in fewer dimensions.\n",
        "\n",
        "## 3. Lack of Semantic Similarity\n",
        "- **Problem:** Traditional methods like one-hot encoding do not capture the semantic similarity between words. For example, \"king\" and \"queen\" are treated as completely unrelated, even though they have similar meanings.\n",
        "- **Solution by Embeddings:** Embeddings capture semantic relationships by placing similar words closer together in the vector space. For instance, \"king\" and \"queen\" will have vectors that are close in the embedding space.\n",
        "\n",
        "## 4. Context Ignorance\n",
        "- **Problem:** Static word representations (e.g., one-hot, BoW, or even early embeddings like Word2Vec) do not take the context of words into account. A word like \"bank\" can have multiple meanings depending on the context, but traditional models treat it as a single entity.\n",
        "- **Solution by Contextualized Embeddings:** Models like BERT and ELMo generate different vectors for the same word depending on its context, effectively capturing its meaning in each specific instance.\n",
        "\n",
        "## 5. Handling Out-of-Vocabulary (OOV) Words\n",
        "- **Problem:** Traditional models struggle with words that were not seen during training (OOV words), as they cannot generate vectors for these unseen words.\n",
        "- **Solution by Subword Embeddings (e.g., FastText):** Subword embeddings break down words into smaller units like character n-grams, allowing the model to generate embeddings for OOV words by composing them from their subwords.\n",
        "\n",
        "## 6. Capturing Long-Range Dependencies\n",
        "- **Problem:** Traditional models, especially those relying on local context (e.g., n-grams), struggle to capture dependencies between words that are far apart in a sentence or document.\n",
        "- **Solution by Contextualized Embeddings and Transformers:** Embeddings from models like BERT and GPT capture long-range dependencies by considering the entire sequence when generating embeddings, thus preserving the relationships between distant words.\n",
        "\n",
        "## 7. Improving Generalization\n",
        "- **Problem:** High-dimensional, sparse representations tend to overfit, especially on small datasets, leading to poor generalization to new, unseen data.\n",
        "- **Solution by Embeddings:** Dense embeddings help in better generalization by learning compact representations that can be effectively used across different tasks and domains.\n",
        "\n",
        "## 8. Difficulty in Transfer Learning\n",
        "- **Problem:** Traditional models often require training from scratch for each new task, which is time-consuming and resource-intensive.\n",
        "- **Solution by Pre-trained Embeddings:** Pre-trained embeddings like those from Word2Vec, GloVe, BERT, etc., can be transferred to new tasks, providing a strong starting point and reducing the amount of labeled data and computation needed.\n",
        "\n",
        "## 9. Capturing Hierarchical Relationships\n",
        "- **Problem:** Simple models may struggle to capture hierarchical or relational information, such as the relationship between \"country\" and \"capital.\"\n",
        "- **Solution by Embeddings:** Advanced embeddings can capture such relationships by learning patterns in the data that reflect these hierarchies, enabling tasks like analogy reasoning (e.g., \"king\" is to \"queen\" as \"man\" is to \"woman\").\n",
        "\n",
        "## 10. Language and Domain Adaptation\n",
        "- **Problem:** Traditional models trained on one domain or language may not perform well on another due to differences in vocabulary, syntax, or semantics.\n",
        "- **Solution by Embeddings:** Embeddings can be fine-tuned for specific domains or languages, allowing for better adaptation and performance across different contexts.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AbBXUjCl6yDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers Democratise the AI Field**\n",
        "\n",
        "### - One architecure for All your Problems\n",
        "### - Models like ***BERT***, ***GPT*** using this and solving multiple problems trained on Big dataset used for specific purposes with bigger knowledge\n",
        "\n",
        "### - Transformers can handle different forms of data and wider ranges of Multimodal challenges for being flexible (Text, Speech, Image)\n",
        "\n"
      ],
      "metadata": {
        "id": "0XHRel-YCV4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Acceleration of GenAI**\n",
        "\n",
        "### - Models like GANS were there but not industry grade\n",
        "### - Textgen Evolved when Transformers came\n",
        "### - Image Generation, AI Image editing"
      ],
      "metadata": {
        "id": "vLlpG1wQDEl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unification of Deep Learning**\n",
        "\n",
        "### - In the past different architecture and models were used for different problems\n",
        "### - But now Transformers are solving a wide range of problems\n",
        "### - Also Transfer Learning is Also Easy with this architecture for the excessive support from vast libraries\n"
      ],
      "metadata": {
        "id": "JW2S_YRXEEYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Origin of the Transformers research Paper\n",
        "\n",
        "### Paper1 : Sequence to Sequence Learning with Neural Network\n",
        "\n",
        "- For Sequence to Sequence task like Machine Learning we use a **Encoder-Decorder Architecture**\n",
        "\n",
        "- In the Both Encoder and Decoder **LSTMs** were used\n",
        "\n",
        "- => The first encoder Inputs the data <br> => Maintains its hidden state <br> => And At the end returns a context Vector(representation of the sentence inputted)\n",
        "- eg: \"I am a student\" <br>[0.25, -0.11, 0.43, 0.67, -0.28, ..., 0.59]\n",
        "\n",
        "- => Now the Decoder processes it and return the answer"
      ],
      "metadata": {
        "id": "ubZPSMGZEZB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paper2: Neural Machine Translation by Jointly Learning to Align and Translation\n",
        "\n",
        "- Here **Attention Mechanism** was Introduced\n",
        "- The Processing of Encoder stays the same.\n",
        "- The Decoder get the context vector(**hidden state**) from each of the units of the decoder bytime parallely and check for the weightage of each of the words (***attention weight***) to actually improve the performance\n",
        "\n",
        "**Note** : **The performance of translation improved after the attension mechanism was introduced to enocoder- decoder architecture even for sentence greater than 30 words**\n",
        "\n",
        "## Limitation :\n",
        "- it is LSTM based as processing happens word to word so the training will be slow\n",
        "\n",
        "- Therefore very difficult to do it for large datasets\n",
        "\n",
        "- So transfer Learning can't happen as we would be able to get a big base model\n",
        "\n",
        "- Ending up doing Model from Scratch which wouldn't be feasible (data collection Cost / Time / Effort )"
      ],
      "metadata": {
        "id": "27pW2fR27k1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally: Attention is all you need (2017)\n",
        "\n",
        "- No LSTMs or RNNs were used\n",
        "\n",
        "- Special Form of Attenion is there(**Self Attention**)\n",
        "\n",
        "- Multiple Components which can be handled Parallely(**Trains Faster**)"
      ],
      "metadata": {
        "id": "hG3mnzq79dHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBEGSD3u4Hnw"
      },
      "outputs": [],
      "source": []
    }
  ]
}